{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test overhead of pyspark UDF\n",
    "\n",
    "Results:\n",
    "1. 80ns per pyspark UDF call\n",
    "2. almost no time cost passing large objects to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff0b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SingleTaskApp\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "346c086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238a6bc",
   "metadata": {},
   "source": [
    "### Declare two simplest UDFs \n",
    "\n",
    "They are simple and use almost no compute time, only need object passing and function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0818f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment(x):\n",
    "    return x + 1\n",
    "\n",
    "spark.udf.register(\"getLen\", len, LongType())\n",
    "spark.udf.register(\"incrementInt\", increment, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10bbe5",
   "metadata": {},
   "source": [
    "### Function call cost\n",
    "\n",
    "Test overhead to invoke python UDF as compared to Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f16ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "# Generate 10 million random 32-bit integers\n",
    "pd.DataFrame({'num': np.random.randint(np.iinfo(np.int32).max+1, size=10000000, dtype=np.int32)}).to_parquet('randint.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c11d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('randint.parquet').createOrReplaceTempView(\"randint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a0b470d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT incrementInt(num) FROM randint\").write.mode(\"overwrite\").parquet(\"incremented.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df940223",
   "metadata": {},
   "source": [
    "takes 9.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16cc5163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT 1+num FROM randint\").write.mode(\"overwrite\").parquet(\"incremented.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213512c",
   "metadata": {},
   "source": [
    "takes 1.3s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824b60b",
   "metadata": {},
   "source": [
    "Thus, the minimum time overhead for each python UDF call is estimated to be 8e-7 seconds (80 nanoseconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab91376",
   "metadata": {},
   "source": [
    "### Data transfer cost\n",
    "\n",
    "Test time cost to pass large data between Spark and python UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf34879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# use spark sql to generate a dataframe with a column 'zeros' containing 100000 all-zero bytes, each of size 100KB, total size 10GB\n",
    "\n",
    "spark.sql(\"SELECT repeat('0', 100000) AS zeros FROM range(100000)\").write.mode(\"overwrite\").parquet(\"zerostr.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c34b981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('zerostr.parquet').createOrReplaceTempView(\"zerostr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b1476b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT LENGTH(zeros) FROM zerostr\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e674fe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT getLen(zeros) FROM zerostr\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both takes 29.7s to calculate length of 10GB binary bytes, thus passing objects to python takes almost no time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
