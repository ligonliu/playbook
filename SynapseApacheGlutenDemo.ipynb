{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%configure -f\n",
        "{\n",
        "    \"driverMemory\": \"8g\",\n",
        "    \"driverCores\": 4,\n",
        "    \"executorMemory\": \"8g\",\n",
        "    \"executorCores\": 4,\n",
        "    \"numExecutors\": 2,\n",
        "    \"conf\":{\n",
        "        \"spark.sql.shuffle.partitions\": 200,\n",
        "        \"spark.dynamicAllocation.enabled\": \"true\",\n",
        "        \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "        \"spark.dynamicAllocation.maxExecutors\": 8,\n",
        "        \"spark.sql.sources.useV1SourceList\":\"true\",\n",
        "        \"spark.sql.join.preferSortMergeJoin\":\"false\",\n",
        "        \"spark.shuffle.manager\": \"org.apache.spark.shuffle.sort.ColumnarShuffleManager\",\n",
        "        \"spark.gluten.ras.enabled\": \"true\",\n",
        "        \"spark.gluten.sql.columnar.backend.velox.bloomFilter.expectedNumItems\": 1000000,\n",
        "        \"spark.gluten.sql.columnar.backend.velox.bloomFilter.numBits\": 8388608,\n",
        "        \"spark.gluten.sql.columnar.backend.velox.bloomFilter.maxNumBits\": 4194304,\n",
        "        \"spark.gluten.velox.awsSdkLogLevel\": \"FATAL\"\n",
        "    }\n",
        "} \n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SmallPool",
              "statement_id": -1,
              "statement_ids": [],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "32",
              "normalized_state": "finished"
            },
            "text/plain": "StatementMeta(SmallPool, 32, -1, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: When setting executor and driver size using %%configure the requested size will be mapped to closest available size which may be bigger than requested. Please use \"configure session\" panel to select directly from available sizes.\nSee https://go.microsoft.com/fwlink/?linkid=2170827\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synapse runtime contains Apache Gluten/Velox, a C++ SQL execution engine.\n",
        "\n",
        "This notebook shows how to enable, allocate memory and check Apache Gluten/Velox.\n",
        "\n",
        "The above configuration is for \"Small\" Spark pool.\n",
        "\n",
        "The following 3 configuration items need to be placed a configuration in Manage --> Apache Spark configurations, and linked to the pool or notebook\n",
        "- \"spark.plugins\":\"org.apache.gluten.GlutenPlugin\"\n",
        "- \"spark.gluten.enabled\": \"true\"\n",
        "- \"spark.memory.offHeap.size\": \"20g\"\n",
        "\n",
        "The last configuration \"spark.memory.offHeap.size\" assigns 20GB of a small node's 28GB to Gluten. Gluten memory is not managed by JVM, thus \"offHeap\". Please adapt memory size for node size.\n",
        "\n",
        "From [Gluten](https://github.com/apache/incubator-gluten/blob/main/docs/Configuration.md)\n",
        "\n",
        "Available memory size of Synapse Spark nodes: \n",
        "- Small: 28g\n",
        "- Medium: 56g\n",
        "- Large: 112g\n",
        "- XLarge: 224g\n",
        "\n",
        "Note: We **cannot** configure spark.executor.memory in Spark configuration. It will have conflict with Synapse's configuration of the same items.\n",
        "\n",
        "From Microsoft: [Magic command for configuring a Spark session](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks#spark-session-config-magic-command)\n",
        "```\n",
        "%%configure\n",
        "{\n",
        "    //You can get a list of valid parameters to configure the session from https://github.com/cloudera/livy#request-body.\n",
        "    \"driverMemory\":\"28g\", // Recommended values: [\"28g\", \"56g\", \"112g\", \"224g\", \"400g\", \"472g\"]\n",
        "    \"driverCores\":4, // Recommended values: [4, 8, 16, 32, 64, 80]\n",
        "    \"executorMemory\":\"28g\",\n",
        "    \"executorCores\":4, \n",
        "    \"jars\":[\"abfs[s]://<file_system>@<account_name>.dfs.core.windows.net/<path>/myjar.jar\",\"wasb[s]://<containername>@<accountname>.blob.core.windows.net/<path>/myjar1.jar\"],\n",
        "    \"conf\":{\n",
        "    //Example of a standard Spark property. To find more available properties, go to https://spark.apache.org/docs/latest/configuration.html#application-properties.\n",
        "        \"spark.driver.maxResultSize\":\"10g\",\n",
        "    //Example of a customized property. You can specify the count of lines that Spark SQL returns by configuring \"livy.rsc.sql.num-rows\".\n",
        "        \"livy.rsc.sql.num-rows\":\"3000\" \n",
        "    }\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check memory configuration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.get('spark.executor.memory'),spark.conf.get('spark.memory.offHeap.size')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SmallPool",
              "statement_id": 4,
              "statement_ids": [
                4
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "32",
              "normalized_state": "finished"
            },
            "text/plain": "StatementMeta(SmallPool, 32, 4, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "('8g', '20g')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check if Gluten/Velox is enabled\n",
        "\n",
        "We can test if Gluten/Velox is in use by EXPLAIN querying a parquet over 5MB. If the plan contains 'Velox', it means Gluten/Velox is in effect"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.sql('EXPLAIN SELECT * FROM parquet.`abfss://******@******.dfs.core.windows.net/disease.parquet`').collect()[0].plan)\n",
        "\n",
        "# example parquet file thanks to https://platform-docs.opentargets.org/data-access/datasets. Put it in your ADLFS G2 container before execution"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SmallPool",
              "statement_id": 5,
              "statement_ids": [
                5
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "32",
              "normalized_state": "finished"
            },
            "text/plain": "StatementMeta(SmallPool, 32, 5, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\nVeloxColumnarToRow\n+- ^(2) FileScanTransformer parquet [id#25,code#26,name#27,description#28,dbXRefs#29,parents#30,synonyms#31,obsoleteTerms#32,obsoleteXRefs#33,children#34,ancestors#35,therapeuticAreas#36,descendants#37,ontology#38] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[abfss://******@*******.dfs.core.windows.net/disease.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,code:string,name:string,description:string,dbXRefs:array<string>,parents:array<s... NativeFilters: []\n\n\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "microsoft": {},
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
